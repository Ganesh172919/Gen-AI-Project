{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031e321a",
   "metadata": {},
   "source": [
    "# Fine-tune GPT-2 for Health Q&A\n",
    "\n",
    "This notebook is a segmented conversion of `re.py` into runnable steps.\n",
    "\n",
    "## Notes\n",
    "- You need a CSV with two columns: **question**, **answer** (or it will be renamed).\n",
    "- Fine-tuning GPT-2 can be slow on CPU; a GPU is strongly recommended.\n",
    "- Hugging Face authentication is required only for pushing the model to the Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fcb2d2",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d80ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets torch pandas huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40561108",
   "metadata": {},
   "source": [
    "## 2) (Optional) Login to Hugging Face\n",
    "Run this if you plan to push the fine-tuned model to the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed16bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This will prompt you for a token in the notebook output area\n",
    "# Create a token at: https://huggingface.co/settings/tokens\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd30bb7d",
   "metadata": {},
   "source": [
    "## 3) Load dataset from CSV\n",
    "Update `DATA_PATH` to your local CSV file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: set your dataset path\n",
    "DATA_PATH = r\"Final data set for llm based health assitance (1).csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna()\n",
    "\n",
    "# Ensure exactly these column names\n",
    "if list(df.columns[:2]) != [\"question\", \"answer\"]:\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = [\"question\", \"answer\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a3bbd",
   "metadata": {},
   "source": [
    "## 4) Tokenizer + prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12234f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# GPT-2 has no pad token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "df[\"prompt\"] = \"Question: \" + df[\"question\"].astype(str) + \"\\nAnswer: \"\n",
    "df[\"response\"] = df[\"answer\"].astype(str)\n",
    "\n",
    "df[[\"prompt\", \"response\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12a329",
   "metadata": {},
   "source": [
    "## 5) PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ae0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GPTQADataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prompt = self.data.loc[index, \"prompt\"]\n",
    "        response = self.data.loc[index, \"response\"]\n",
    "        full_text = prompt + response\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            # Causal LM training: labels are the same as input_ids\n",
    "            \"labels\": input_ids.clone(),\n",
    "        }\n",
    "\n",
    "# Quick sanity check\n",
    "sample = GPTQADataset(df.head(2), tokenizer)[0]\n",
    "{k: v.shape for k, v in sample.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26c137",
   "metadata": {},
   "source": [
    "## 6) Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd526ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df))\n",
    "train_df = df.iloc[:train_size].copy()\n",
    "val_df = df.iloc[train_size:].copy()\n",
    "\n",
    "train_dataset = GPTQADataset(train_df, tokenizer)\n",
    "val_dataset = GPTQADataset(val_df, tokenizer)\n",
    "\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d599b",
   "metadata": {},
   "source": [
    "## 7) Fine-tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff21cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cccf76",
   "metadata": {},
   "source": [
    "## 8) Save the fine-tuned model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OUTPUT_DIR = \"./fine_tuned_gpt\"\n",
    "model.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(f\"Model saved to: {MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f5ed9",
   "metadata": {},
   "source": [
    "## 9) (Optional) Upload model to Hugging Face Hub\n",
    "Make sure you ran the login step first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "HF_REPO = \"Ganesh19128734/fine-tuned-gpt-project\"  # TODO: change to your username/repo\n",
    "\n",
    "upload_model = AutoModelForCausalLM.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "upload_tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "\n",
    "upload_model.push_to_hub(HF_REPO)\n",
    "upload_tokenizer.push_to_hub(HF_REPO)\n",
    "\n",
    "print(f\"Model uploaded to: https://huggingface.co/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69cfd5",
   "metadata": {},
   "source": [
    "## 10) Test the fine-tuned model\n",
    "You can test either your local saved model (`MODEL_OUTPUT_DIR`) or your HF repo id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c794aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_answer(question: str, model_path: str = MODEL_OUTPUT_DIR) -> str:\n",
    "    local_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    local_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    local_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = local_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=local_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = local_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"Answer:\" in generated_text:\n",
    "        answer = generated_text.split(\"Answer:\", 1)[1].strip()\n",
    "    else:\n",
    "        answer = generated_text[len(prompt):].strip()\n",
    "\n",
    "    # Optional cleanup carried over from the original script\n",
    "    if \"?\" in answer:\n",
    "        answer = answer.split(\"?\", 1)[-1].strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "sample_question = \"What are the potential side effects of ibuprofen?\"\n",
    "response = get_answer(sample_question)\n",
    "\n",
    "print(\"Question:\", sample_question)\n",
    "print(\"Answer:\", response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
